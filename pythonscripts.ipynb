{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bCqz5BzObkoS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os #for file management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HiJ3-XUibnEu"
   },
   "outputs": [],
   "source": [
    "base_dir = '../train' #setting the base_dir variable to the location of the dataset containing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9RDjXSRFbnHH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 191 images belonging to 3 classes.\n",
      "Found 47 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#now we will do some preprocessing, i.e we are preparing the raw data to make it suitable for a building and training models\n",
    "IMAGE_SIZE = 224 #image size that we are going to set the images in the dataset to.\n",
    "BATCH_SIZE = 64 #the number of images we are inputting into the neural network at once.\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator( #preprocessing our image\n",
    "    rescale = 1./255, #firstly, rescaling it to 1/255 which will make the file size smaller, hence reducing the training time\n",
    "    validation_split=0.2 #secondly, normally a dataset has a test set and a training set, \n",
    "    #validation set is normally to test our neural network,which would give us a measure of accuracy on how well the neural network will do on the predictions.\n",
    "    #here we are telling keras to use 20% for validation and 80% training\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory( #training generator\n",
    "    base_dir, #the directory having the fruits and vegetable photos\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),  #converting images to 224 by 224\n",
    "    batch_size = BATCH_SIZE, #images getting inputed into the neural network through each epoch or each step\n",
    "    subset='training' #the name we will call it\n",
    ")\n",
    "val_generator = datagen.flow_from_directory(  #validation generator\n",
    "    base_dir, \n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")\n",
    "#So as we can see from below, our training generator dataset 2872 images and the validation generator dataset has 709 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L8v9pUx0bnJh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.ipynb_checkpoints': 0, 'nao': 1, 'sim': 2}\n"
     ]
    }
   ],
   "source": [
    "#Next we have to create a labels.txt file that will hold all our labels (important for Flutter)\n",
    "print(train_generator.class_indices) #prints every single key and class of that dataset\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys())) #print all these keys as a list of labels into a text file called labels.txt\n",
    "with open('labels.txt', 'w') as f: #writes to the labels.txt file, and if it doesnt exists, it creates one, and if it does exist, it will overrite it. (thats what 'w' is for)\n",
    "    f.write(labels)\n",
    "\n",
    "#preprocessing of raw data is hence complete and now its time to build our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6Zwet3CbbnL-"
   },
   "outputs": [],
   "source": [
    "#building a neural network using transfer learning method where we take a pretrained neural network called MobileNetV2 which is a convolutional neural network architecture that seeks to perform well on mobile devices and can predict up to 80 different classes\n",
    "#we are going to have a base model on top of which we are going to add pre trained neural network to have it predict the classes we want\n",
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3) \n",
    "base_model = tf.keras.applications.MobileNetV2( #grabbing pretrained neural network of choice\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False, #this will freeze all the weights, because we dont have to retrain and change the weights, instead just add on to the MobileNetV2 CNN, so it clasiffies 5 classes instead of 80\n",
    "    weights='imagenet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GuYklq4SbnOU"
   },
   "outputs": [],
   "source": [
    "base_model.trainable=False #this freezes all the neurons for our base model\n",
    "model = tf.keras.Sequential([ #neural networks act in a sequence of layers, so we add layers as we want\n",
    "  base_model,\n",
    "  tf.keras.layers.Conv2D(32,3, activation = 'relu'), #This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. Bascially, it trying to understand the patterns of the image\n",
    "  tf.keras.layers.Dropout(0.2), #This layer prevents Neural Networks from Overfitting, i.e being too precise to a point where the NN is only able to recognize images that are present in the dataset\n",
    "  tf.keras.layers.GlobalAveragePooling2D(), #This layer calculates the average output of each feature map in the previous layer, thus reducing the data significantly and preparing the model for the final layer\n",
    "  tf.keras.layers.Dense(3, #no.of classes\n",
    "                        activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bIww3EHpbnQn"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), #Adam is a popular optimiser, designed specifically for training deep neural networks\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c-WUCFsPbnS6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "3/3 [==============================] - 10s 3s/step - loss: 0.2675 - accuracy: 0.8691 - val_loss: 0.6822 - val_accuracy: 0.9787\n",
      "Epoch 2/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6831 - accuracy: 0.9791 - val_loss: 0.5894 - val_accuracy: 0.9787\n",
      "Epoch 3/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.3657 - accuracy: 0.9791 - val_loss: 0.5558 - val_accuracy: 0.9149\n",
      "Epoch 4/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.3074e-06 - accuracy: 1.0000 - val_loss: 2.5030 - val_accuracy: 0.8085\n",
      "Epoch 5/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 4.3555 - val_accuracy: 0.8085\n",
      "Epoch 6/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 5.7920 - val_accuracy: 0.7872\n",
      "Epoch 7/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.9958e-08 - accuracy: 1.0000 - val_loss: 6.8326 - val_accuracy: 0.7872\n",
      "Epoch 8/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 6.6158e-08 - accuracy: 1.0000 - val_loss: 7.5930 - val_accuracy: 0.7872\n",
      "Epoch 9/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.5108e-06 - accuracy: 1.0000 - val_loss: 8.1561 - val_accuracy: 0.7872\n",
      "Epoch 10/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.4559e-06 - accuracy: 1.0000 - val_loss: 8.5779 - val_accuracy: 0.7872\n",
      "Epoch 11/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 6.7954e-05 - accuracy: 1.0000 - val_loss: 8.8916 - val_accuracy: 0.7872\n",
      "Epoch 12/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.4574e-04 - accuracy: 1.0000 - val_loss: 9.1099 - val_accuracy: 0.7872\n",
      "Epoch 13/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.8284e-04 - accuracy: 1.0000 - val_loss: 9.2321 - val_accuracy: 0.7872\n",
      "Epoch 14/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.1676e-05 - accuracy: 1.0000 - val_loss: 9.3233 - val_accuracy: 0.7872\n",
      "Epoch 15/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.2201e-04 - accuracy: 1.0000 - val_loss: 9.3785 - val_accuracy: 0.7872\n",
      "Epoch 16/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.0878e-05 - accuracy: 1.0000 - val_loss: 9.4186 - val_accuracy: 0.7872\n",
      "Epoch 17/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.5920e-06 - accuracy: 1.0000 - val_loss: 9.4467 - val_accuracy: 0.7872\n",
      "Epoch 18/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 6.2113e-05 - accuracy: 1.0000 - val_loss: 9.4626 - val_accuracy: 0.7872\n",
      "Epoch 19/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.6998e-06 - accuracy: 1.0000 - val_loss: 9.4724 - val_accuracy: 0.7872\n",
      "Epoch 20/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 4.8202e-06 - accuracy: 1.0000 - val_loss: 9.4788 - val_accuracy: 0.7872\n",
      "Epoch 21/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.9393e-06 - accuracy: 1.0000 - val_loss: 9.4832 - val_accuracy: 0.7872\n",
      "Epoch 22/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.8099e-06 - accuracy: 1.0000 - val_loss: 9.4861 - val_accuracy: 0.7872\n",
      "Epoch 23/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.6644e-05 - accuracy: 1.0000 - val_loss: 9.4871 - val_accuracy: 0.7872\n",
      "Epoch 24/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 4.0505e-07 - accuracy: 1.0000 - val_loss: 9.4867 - val_accuracy: 0.7872\n",
      "Epoch 25/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.5340e-06 - accuracy: 1.0000 - val_loss: 9.4862 - val_accuracy: 0.7872\n",
      "Epoch 26/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.4600e-06 - accuracy: 1.0000 - val_loss: 9.4852 - val_accuracy: 0.7872\n",
      "Epoch 27/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 5.5546e-07 - accuracy: 1.0000 - val_loss: 9.4844 - val_accuracy: 0.7872\n",
      "Epoch 28/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.5633e-06 - accuracy: 1.0000 - val_loss: 9.4835 - val_accuracy: 0.7872\n",
      "Epoch 29/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.5883e-06 - accuracy: 1.0000 - val_loss: 9.4822 - val_accuracy: 0.7872\n",
      "Epoch 30/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 7.8781e-06 - accuracy: 1.0000 - val_loss: 9.4808 - val_accuracy: 0.7872\n",
      "Epoch 31/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.0559e-06 - accuracy: 1.0000 - val_loss: 9.4789 - val_accuracy: 0.7872\n",
      "Epoch 32/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 8.6064e-07 - accuracy: 1.0000 - val_loss: 9.4775 - val_accuracy: 0.7872\n",
      "Epoch 33/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 4.2689e-07 - accuracy: 1.0000 - val_loss: 9.4763 - val_accuracy: 0.7872\n",
      "Epoch 34/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.2775e-06 - accuracy: 1.0000 - val_loss: 9.4754 - val_accuracy: 0.7872\n",
      "Epoch 35/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.3041e-06 - accuracy: 1.0000 - val_loss: 9.4741 - val_accuracy: 0.7872\n",
      "Epoch 36/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.6782e-06 - accuracy: 1.0000 - val_loss: 9.4730 - val_accuracy: 0.7872\n",
      "Epoch 37/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.7235e-06 - accuracy: 1.0000 - val_loss: 9.4716 - val_accuracy: 0.7872\n",
      "Epoch 38/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.0413e-06 - accuracy: 1.0000 - val_loss: 9.4700 - val_accuracy: 0.7872\n",
      "Epoch 39/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.4909e-06 - accuracy: 1.0000 - val_loss: 9.4686 - val_accuracy: 0.7872\n",
      "Epoch 40/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 6.4034e-07 - accuracy: 1.0000 - val_loss: 9.4674 - val_accuracy: 0.7872\n",
      "Epoch 41/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 6.4657e-07 - accuracy: 1.0000 - val_loss: 9.4665 - val_accuracy: 0.7872\n",
      "Epoch 42/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.3356e-07 - accuracy: 1.0000 - val_loss: 9.4657 - val_accuracy: 0.7872\n",
      "Epoch 43/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.7898e-07 - accuracy: 1.0000 - val_loss: 9.4652 - val_accuracy: 0.7872\n",
      "Epoch 44/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.9295e-06 - accuracy: 1.0000 - val_loss: 9.4643 - val_accuracy: 0.7872\n",
      "Epoch 45/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.9285e-07 - accuracy: 1.0000 - val_loss: 9.4636 - val_accuracy: 0.7872\n",
      "Epoch 46/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.6295e-06 - accuracy: 1.0000 - val_loss: 9.4628 - val_accuracy: 0.7872\n",
      "Epoch 47/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.7509e-07 - accuracy: 1.0000 - val_loss: 9.4621 - val_accuracy: 0.7872\n",
      "Epoch 48/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.7551e-06 - accuracy: 1.0000 - val_loss: 9.4614 - val_accuracy: 0.7872\n",
      "Epoch 49/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.5682e-06 - accuracy: 1.0000 - val_loss: 9.4598 - val_accuracy: 0.7872\n",
      "Epoch 50/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.3390e-07 - accuracy: 1.0000 - val_loss: 9.4586 - val_accuracy: 0.7872\n",
      "Epoch 51/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.8522e-06 - accuracy: 1.0000 - val_loss: 9.4575 - val_accuracy: 0.7872\n",
      "Epoch 52/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 8.9372e-07 - accuracy: 1.0000 - val_loss: 9.4564 - val_accuracy: 0.7872\n",
      "Epoch 53/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 7.8388e-07 - accuracy: 1.0000 - val_loss: 9.4554 - val_accuracy: 0.7872\n",
      "Epoch 54/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.0799e-06 - accuracy: 1.0000 - val_loss: 9.4542 - val_accuracy: 0.7872\n",
      "Epoch 55/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.9958e-07 - accuracy: 1.0000 - val_loss: 9.4532 - val_accuracy: 0.7872\n",
      "Epoch 56/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 2.5839e-07 - accuracy: 1.0000 - val_loss: 9.4525 - val_accuracy: 0.7872\n",
      "Epoch 57/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 2.4653e-07 - accuracy: 1.0000 - val_loss: 9.4519 - val_accuracy: 0.7872\n",
      "Epoch 58/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 3.3515e-07 - accuracy: 1.0000 - val_loss: 9.4514 - val_accuracy: 0.7872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.2894e-06 - accuracy: 1.0000 - val_loss: 9.4508 - val_accuracy: 0.7872\n",
      "Epoch 60/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 5.7980e-07 - accuracy: 1.0000 - val_loss: 9.4502 - val_accuracy: 0.7872\n",
      "Epoch 61/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 1.3655e-06 - accuracy: 1.0000 - val_loss: 9.4496 - val_accuracy: 0.7872\n",
      "Epoch 62/64\n",
      "3/3 [==============================] - 6s 2s/step - loss: 4.8099e-06 - accuracy: 1.0000 - val_loss: 9.4486 - val_accuracy: 0.7872\n",
      "Epoch 63/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 1.2607e-07 - accuracy: 1.0000 - val_loss: 9.4473 - val_accuracy: 0.7872\n",
      "Epoch 64/64\n",
      "3/3 [==============================] - 5s 2s/step - loss: 3.6448e-07 - accuracy: 1.0000 - val_loss: 9.4462 - val_accuracy: 0.7872\n"
     ]
    }
   ],
   "source": [
    "epochs = 64 #higher the epochs, more accurate is the NN, however it could cause Overfitting, if too high\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs = epochs, \n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bMJhJkpZb5dK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n",
      "WARNING:absl:Function `signature_wrapper` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4wjw99pr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4wjw99pr/assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "#now that we have our neural network trained with tensorflow and keras, we can export it \n",
    "saved_model_dir = '' #means current directory\n",
    "tf.saved_model.save(model, saved_model_dir) #saves to the current directory\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) \n",
    "tflite_model = converter.convert() #converts our model into a .tflite model which flutter uses for ondevice machine learning\n",
    "\n",
    "with open('model.tflite', 'wb') as f: #to write the converted model into a file, written as binary so add 'wb' instead of 'w'\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "y1nznkgIb5fW"
   },
   "outputs": [],
   "source": [
    "#use below codes to download files locally if using google colab\n",
    "#from google.colab import files\n",
    "#files.download('model.tflite')\n",
    "#files.download('labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMjqE22Bu5IS3NSvOUU3suL",
   "collapsed_sections": [],
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
